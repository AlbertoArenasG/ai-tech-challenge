"""Business logic for interacting with the LLM-powered commercial agent."""
from __future__ import annotations

import logging
from pathlib import Path
from typing import Any

from openai import OpenAI

from app.config import Settings
from app.domain.models import FinancingPlan, Recommendation
from app.domain.schemas import ChatRequest, ChatResponse
from app.services.catalog_service import CatalogService
from app.services.finance_service import calculate_financing

LOGGER = logging.getLogger(__name__)
DEFAULT_VALUE_PROPOSITION = (
    "Kavak ofrece autos seminuevos inspeccionados con garantías, financiamiento flexible "
    "y acompañamiento comercial end-to-end para clientes en Latinoamérica."
)


class LLMClient:
    """Thin wrapper around OpenAI's Chat Completions API."""

    def __init__(self, api_key: str | None, model: str) -> None:
        self.model = model
        self._client: OpenAI | None = None
        if api_key:
            self._client = OpenAI(api_key=api_key)

    def generate(self, system_prompt: str, user_prompt: str) -> str:
        if not self._client:
            return "LLM client not configured. Please provide an OPENAI_API_KEY."

        try:
            response = self._client.chat.completions.create(
                model=self.model,
                temperature=0.2,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
            )
        except Exception as exc:  # pragma: no cover - network call
            LOGGER.warning("LLM call failed: %s", exc)
            return "I'm unable to reach the language model right now, but here is a curated response."

        message = response.choices[0].message.content
        return message or "No response generated by the language model."


class CommercialAgentService:
    """Prepare prompts and mediate interactions with the LLM."""

    def __init__(
        self,
        catalog_service: CatalogService,
        settings: Settings,
        knowledge_base_path: str | None = None,
    ) -> None:
        self.catalog_service = catalog_service
        self.settings = settings
        self.llm_client = LLMClient(settings.openai_api_key, settings.openai_model)
        self.knowledge_base = self._load_knowledge_base(knowledge_base_path) or DEFAULT_VALUE_PROPOSITION

    def answer(self, request: ChatRequest) -> ChatResponse:
        """Route a chat request through catalog, finance and LLM components."""
        recommendations = self._build_recommendations(request.preferences)
        financing_plan = self._build_financing_plan(request)

        assistant_message = self._generate_agent_reply(
            user_message=request.message,
            recommendations=recommendations,
            financing_plan=financing_plan,
        )

        return ChatResponse(
            message=assistant_message,
            recommendations=recommendations,
            financing_plan=financing_plan,
        )

    def _build_recommendations(self, preferences: dict[str, Any] | None) -> list[Recommendation]:
        if not preferences:
            return []

        cars = self.catalog_service.search_cars(preferences)
        return [Recommendation(car=car, reason="Matches stated preferences") for car in cars[:3]]

    @staticmethod
    def _load_knowledge_base(path: str | None) -> str | None:
        if not path:
            return None
        file_path = Path(path)
        if not file_path.exists():
            LOGGER.warning("Value proposition file not found at %s", path)
            return None
        return file_path.read_text(encoding="utf-8")

    def _build_financing_plan(self, request: ChatRequest) -> FinancingPlan | None:
        if not request.financing:
            return None

        financing = request.financing
        return calculate_financing(
            price=financing.car_price,
            down_payment=financing.down_payment,
            years=financing.years,
        )

    def _generate_agent_reply(
        self,
        user_message: str,
        recommendations: list[Recommendation],
        financing_plan: FinancingPlan | None,
    ) -> str:
        system_prompt = (
            "Eres un agente comercial de Kavak. Solo puedes responder utilizando la información "
            "incluida en el contexto proporcionado. Si el cliente pregunta sobre la propuesta de valor, "
            "debes citar únicamente lo que aparece en el archivo oficial. No inventes datos ni hagas "
            "suposiciones, y si falta información responde que no tienes esa información disponible."
        )

        context_sections = [self.knowledge_base]
        if recommendations:
            rec_lines = [
                f"- {rec.car.make} {rec.car.model} {rec.car.year} at MXN {rec.car.price:,.0f}"
                for rec in recommendations
            ]
            context_sections.append("Recommended vehicles:\n" + "\n".join(rec_lines))

        if financing_plan:
            context_sections.append(
                f"Financing plan: {financing_plan.months} months, MXN {financing_plan.monthly_payment} per month, "
                f"total paid {financing_plan.total_paid}."
            )

        context = "\n\n".join(context_sections)
        user_prompt = (
            "Mensaje del cliente: "
            f"{user_message}\n\nContexto autorizado:\n{context}\n\n"
            "Instrucciones: responde en español, cita solamente datos que estén en el contexto y, si no "
            "existe la información solicitada, aclara que no está disponible."
        )
        return self.llm_client.generate(system_prompt, user_prompt)
